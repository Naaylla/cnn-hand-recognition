{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd44d2a",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES & DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1180bea",
   "metadata": {},
   "source": [
    "- ImageFolder is a dataset class provided by torchvision.datasets that helps load images organized in folders automatically.\n",
    "- DataLoader is a PyTorch utility that loads data from a dataset and provides batches of samples during training or evaluation.\n",
    "\n",
    "## Download the dataset from here : \n",
    "\n",
    "[HG14 (HandGesture14) dataset](https://www.kaggle.com/datasets/gulerosman/hg14-handgesture14-dataset/code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Path(\"./HG14\")\n",
    "output = Path(\"./HG14_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b9535",
   "metadata": {},
   "source": [
    "# **SPLITTING DATA**\n",
    "\n",
    "10% random images from 14000 images were selected\n",
    "from each class, a total of 1400 images were reserved for\n",
    "testing.\n",
    "\n",
    "Then, 20% of the remaining 12600 images (2520\n",
    "images) were randomly divided for validation. \n",
    "\n",
    "The remaining\n",
    "10080 images were used for the train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fixing the seed will give us the same random data each time (praticale for comparing)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for class_folder in sorted(dataset.iterdir()):\n",
    "    if class_folder.is_dir():\n",
    "        images = list(class_folder.glob(\"*.jpg\"))\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        total = len(images)\n",
    "        test_count = int(0.10 * total) # 10% pour testing \n",
    "        val_count = int(0.20 * (total - test_count)) # 20% pour validation \n",
    "\n",
    "        test_imgs = images[:test_count]\n",
    "        val_imgs = images[test_count:test_count + val_count]\n",
    "        train_imgs = images[test_count + val_count:]\n",
    "\n",
    "        split_dict = {\n",
    "            \"training\": train_imgs,\n",
    "            \"validation\": val_imgs,\n",
    "            \"testing\": test_imgs,\n",
    "        }\n",
    "\n",
    "        # creating folders here \n",
    "        for split_name, split_images in split_dict.items():\n",
    "            output_dir = output / split_name / class_folder.name\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for img_path in split_images:\n",
    "                shutil.copy(img_path, output_dir / img_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d719f09",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326caff8",
   "metadata": {},
   "source": [
    "Global Variables : \n",
    "- Resizing images to 128x128. (all images in the batch need to be the same size to stack them into a tensor)\n",
    "- Splitting into batch sizes of 20, we don’t feed the entire dataset at once, instead, we divide it into smaller groups of 20 samples that the model processes one batch at a time.\n",
    "- We chose 50 epochs, which means the model will see the entire dataset 50 times during training.\n",
    "- num_classes = 14, it means the model will output 14 scores, each representing how likely the input belongs to each of those 14 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (128, 128)\n",
    "batch_size = 20\n",
    "epochs = 50\n",
    "num_classes = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        # those are standard normalization values \n",
    "        mean=[0.485, 0.456, 0.406],  \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee64a37",
   "metadata": {},
   "source": [
    "Loading data sets and applying transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10861285",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root='HG14_split/training', transform=transform)\n",
    "val_dataset   = ImageFolder(root='HG14_split/validation', transform=transform)\n",
    "test_dataset  = ImageFolder(root='HG14_split/testing', transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259b0da",
   "metadata": {},
   "source": [
    "Creating data loaders (splitting dataset into small batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e45351",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "We're using transfer learning, it means we don’t start training from scratch, instead, we start with a model that already knows useful image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "mobilenet_v2 = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c925645",
   "metadata": {},
   "source": [
    "We can see here for example the structure of the vgg16 model,\n",
    "it has a part of feature extraction (convolutional layers)\n",
    "and a part of classifier (fully connected layers)\n",
    "\n",
    "\n",
    "the classifier takes the features learned by the convolutional layers, Combine them And output a prediction.\n",
    "This is where we're gonna operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b647b4",
   "metadata": {},
   "source": [
    "Modifying the last layer of the classifier for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 has a 7 layer classifier, counting from 0 to 6, so we take the last layer inputs (in-features) and the output features are the number of classes 14\n",
    "\n",
    "vgg16.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "vgg19.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "mobilenet_v2.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ef725",
   "metadata": {},
   "source": [
    "We freeze the feature extracting layer (the models already know how to extract features so no need to do it again)\n",
    "\n",
    "The attribute requires_grad is a flag that tells PyTorch if True: This parameter will be updated during training because PyTorch will calculate its gradients.\n",
    "If False: This parameter will not be updated, it’s 'frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_features(model):\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "freeze_features(vgg16)\n",
    "freeze_features(vgg19)\n",
    "freeze_features(mobilenet_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2daf17",
   "metadata": {},
   "source": [
    "According to the article: \n",
    "\n",
    "We're gonna modify our classifier like this: \n",
    "\n",
    "We Use a dropout of 0.5% (0.005) meanings we randomly \"drops\" (disables) 0.5% of neurons during training to avoid overfitting.\n",
    "We Reduce neuron to 14\n",
    "We Use ReLU and Softmax as activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 512),   \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(512, num_classes), \n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "vgg19.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "mobilenet_v2.classifier = nn.Sequential(\n",
    "    nn.Linear(1280, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915573ba",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training our models either in or gpu or cpu if there's no gpu (i have a gpu) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    # initializing variables to keep track of loss, correct predciitions, and total of examples seen so far\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / len(dataloader), 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9efd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffac79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader)\n",
    "        val_acc = validate(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} - Train Acc: {train_acc:.2f}% - Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5cb42",
   "metadata": {},
   "source": [
    "Once we trained our models, we save them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523309d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60ba1619",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining VGG16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m vgg16 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgg16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m save_model(vgg16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg16_hg14.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining VGG19\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 6\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% - Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, criterion, dataloader)\u001b[0m\n\u001b[0;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Bleu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bleu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bleu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "print(\"Training VGG16\")\n",
    "vgg16 = train_model(vgg16, train_loader, val_loader, epochs)\n",
    "save_model(vgg16, 'vgg16_hg14.pth')\n",
    "\n",
    "print(\"Training VGG19\")\n",
    "vgg19 = train_model(vgg19, train_loader, val_loader, epochs)\n",
    "save_model(vgg19, 'vgg19_hg14.pth')\n",
    "\n",
    "print(\"Training MobileNetV2\")\n",
    "mobilenet_v2 = train_model(mobilenet_v2, train_loader, val_loader, epochs)\n",
    "save_model(mobilenet_v2, 'mobilenet_v2_hg14.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a227cf",
   "metadata": {},
   "source": [
    "# Dirichlet Ensemble Learning\n",
    "\n",
    "When we have multiple models trained on the same task, instead of just picking one or averaging their outputs evenly, we can combine their predictions using weights that come from a Dirichlet distribution.\n",
    "\n",
    "So now, time for testing and combining the three models ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29450ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22864474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "vgg16 = load_model(vgg16, 'vgg16_hg14.pth')\n",
    "vgg19 = load_model(vgg19, 'vgg19_hg14.pth')\n",
    "mobilenet_v2 = load_model(mobilenet_v2, 'mobilenet_v2_hg14.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c943e",
   "metadata": {},
   "source": [
    "Testing models individually is used to compute performances of the different models (we won't do it because we already know how the models perfom in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test each model\n",
    "# acc_vgg16, preds_vgg16, labels = test_model(vgg16, test_loader)\n",
    "# acc_vgg19, preds_vgg19, _ = test_model(vgg19, test_loader)\n",
    "# acc_mobilenet, preds_mobilenet, _ = test_model(mobilenet_v2, test_loader)\n",
    "\n",
    "# print(f\"Test Accuracy - VGG16: {acc_vgg16:.2f}%\")\n",
    "# print(f\"Test Accuracy - VGG19: {acc_vgg19:.2f}%\")\n",
    "# print(f\"Test Accuracy - MobileNetV2: {acc_mobilenet:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21d52c",
   "metadata": {},
   "source": [
    "We can  visualise the confusion matrix, A table used to evaluate the performance of a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf54fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(labels, preds_vgg16)  \n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "# disp.plot()\n",
    "# plt.title(\"Confusion Matrix - VGG16\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_ensemble(models, test_loader, num_runs=10):\n",
    "    all_accuracies = []\n",
    "    final_predictions = None\n",
    "    final_labels = None\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        all_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            probs = []\n",
    "            with torch.no_grad():\n",
    "                for images, _ in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    probs.append(outputs.cpu().numpy())\n",
    "            all_probs.append(np.concatenate(probs, axis=0)) \n",
    "\n",
    "        weights = np.random.dirichlet(np.ones(len(models)))\n",
    "        print(f\"Run {run+1} Dirichlet weights: {weights}\")\n",
    "\n",
    "        weighted_probs = sum(w * p for w, p in zip(weights, all_probs))\n",
    "        predictions = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "        all_labels = []\n",
    "        for _, labels in test_loader:\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, predictions)\n",
    "        print(f\"Run {run+1} Accuracy: {accuracy * 100:.2f}%\")\n",
    "        all_accuracies.append(accuracy * 100)\n",
    "\n",
    "        if run == num_runs - 1:\n",
    "            final_predictions = predictions\n",
    "            final_labels = all_labels\n",
    "  \n",
    "    avg_acc = np.mean(all_accuracies)\n",
    "    std_acc = np.std(all_accuracies)\n",
    "    print(f\"\\nAverage Accuracy over {num_runs} runs: {avg_acc:.2f}% ± {std_acc:.2f}%\")\n",
    "\n",
    "    return all_accuracies, avg_acc, std_acc, final_predictions, final_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1115ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies, avg_acc, std_acc, final_preds, final_labels = dirichlet_ensemble(models, test_loader, num_runs=10)\n",
    "\n",
    "print(f\"Final Ensemble Accuracy: {avg_acc:.2f}% ± {std_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(final_labels, final_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix for Dirichlet Ensemble\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
