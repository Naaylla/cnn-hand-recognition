{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd44d2a",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES & DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8ffd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1180bea",
   "metadata": {},
   "source": [
    "- ImageFolder is a dataset class provided by torchvision.datasets that helps load images organized in folders automatically.\n",
    "- DataLoader is a PyTorch utility that loads data from a dataset and provides batches of samples during training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e705f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Path(\"./HG14\")\n",
    "output = Path(\"./HG14_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b9535",
   "metadata": {},
   "source": [
    "# **SPLITTING DATA**\n",
    "\n",
    "10% random images from 14000 images were selected\n",
    "from each class, a total of 1400 images were reserved for\n",
    "testing.\n",
    "\n",
    "Then, 20% of the remaining 12600 images (2520\n",
    "images) were randomly divided for validation. \n",
    "\n",
    "The remaining\n",
    "10080 images were used for the train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fixing the seed will give us the same random data each time (praticale for comparing)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f92a700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for class_folder in sorted(dataset.iterdir()):\n",
    "    if class_folder.is_dir():\n",
    "        images = list(class_folder.glob(\"*.jpg\"))\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        total = len(images)\n",
    "        test_count = int(0.10 * total) # 10% pour testing \n",
    "        val_count = int(0.20 * (total - test_count)) # 20% pour validation \n",
    "\n",
    "        test_imgs = images[:test_count]\n",
    "        val_imgs = images[test_count:test_count + val_count]\n",
    "        train_imgs = images[test_count + val_count:]\n",
    "\n",
    "        split_dict = {\n",
    "            \"training\": train_imgs,\n",
    "            \"validation\": val_imgs,\n",
    "            \"testing\": test_imgs,\n",
    "        }\n",
    "\n",
    "        # creating folders here \n",
    "        for split_name, split_images in split_dict.items():\n",
    "            output_dir = output / split_name / class_folder.name\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for img_path in split_images:\n",
    "                shutil.copy(img_path, output_dir / img_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d719f09",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326caff8",
   "metadata": {},
   "source": [
    "Global Variables : \n",
    "- Resizing images to 128x128. (all images in the batch need to be the same size to stack them into a tensor)\n",
    "- Splitting into batch sizes of 20, we don’t feed the entire dataset at once, instead, we divide it into smaller groups of 20 samples that the model processes one batch at a time.\n",
    "- We chose 50 epochs, which means the model will see the entire dataset 50 times during training.\n",
    "- num_classes = 14, it means the model will output 14 scores, each representing how likely the input belongs to each of those 14 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c17b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (128, 128)\n",
    "batch_size = 20\n",
    "epochs = 50\n",
    "num_classes = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81b2c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        # those are standard normalization values \n",
    "        mean=[0.485, 0.456, 0.406],  \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee64a37",
   "metadata": {},
   "source": [
    "Loading data sets and applying transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10861285",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'HG14_split/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHG14_split/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m val_dataset   \u001b[38;5;241m=\u001b[39m ImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHG14_split/val\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      3\u001b[0m test_dataset  \u001b[38;5;241m=\u001b[39m ImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHG14_split/test\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[1;32mc:\\Users\\Bleu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\Bleu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\Bleu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bleu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'HG14_split/train'"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(root='HG14_split/training', transform=transform)\n",
    "val_dataset   = ImageFolder(root='HG14_split/validation', transform=transform)\n",
    "test_dataset  = ImageFolder(root='HG14_split/testing', transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259b0da",
   "metadata": {},
   "source": [
    "Creating data loaders (splitting dataset into small batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e45351",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "We're using transfer learning, it means we don’t start training from scratch, instead, we start with a model that already knows useful image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Bleu/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\Bleu/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "4.6%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\Bleu/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "mobilenet_v2 = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c925645",
   "metadata": {},
   "source": [
    "We can see here for example the structure of the vgg16 model,\n",
    "it has a part of feature extraction (convolutional layers)\n",
    "and a part of classifier (fully connected layers)\n",
    "\n",
    "\n",
    "the classifier takes the features learned by the convolutional layers, Combine them And output a prediction.\n",
    "This is where we're gonna operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b647b4",
   "metadata": {},
   "source": [
    "Modifying the last layer of the classifier for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 has a 7 layer classifier, counting from 0 to 6, so we take the last layer inputs (in-features) and the output features are the number of classes 14\n",
    "\n",
    "vgg16.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "vgg19.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "mobilenet_v2.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ef725",
   "metadata": {},
   "source": [
    "We freeze the feature extracting layer (the models already know how to extract features so no need to do it again)\n",
    "\n",
    "The attribute requires_grad is a flag that tells PyTorch if True: This parameter will be updated during training because PyTorch will calculate its gradients.\n",
    "If False: This parameter will not be updated, it’s 'frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_features(model):\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "freeze_features(vgg16)\n",
    "freeze_features(vgg19)\n",
    "freeze_features(mobilenet_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2daf17",
   "metadata": {},
   "source": [
    "According to the article: \n",
    "\n",
    "We're gonna modify our classifier like this: \n",
    "\n",
    "We Use a dropout of 0.5% (0.005) meanings we randomly \"drops\" (disables) 0.5% of neurons during training to avoid overfitting.\n",
    "We Reduce neuron to 14\n",
    "We Use ReLU and Softmax as activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 512),   \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(512, num_classes), \n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "vgg19.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "mobilenet_v2.classifier = nn.Sequential(\n",
    "    nn.Linear(1280, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.005),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915573ba",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training our models either in or gpu or cpu if there's no gpu (i have a gpu) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    # initializing variables to keep track of loss, correct predciitions, and total of examples seen so far\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / len(dataloader), 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9efd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffac79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader)\n",
    "        val_acc = validate(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} - Train Acc: {train_acc:.2f}% - Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5cb42",
   "metadata": {},
   "source": [
    "Once we trained our models, we save them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523309d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba1619",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(\"Training VGG16\")\n",
    "vgg16 = train_model(vgg16, train_loader, val_loader, epochs)\n",
    "save_model(vgg16, 'vgg16_hg14.pth')\n",
    "\n",
    "print(\"Training VGG19\")\n",
    "vgg19 = train_model(vgg19, train_loader, val_loader, epochs)\n",
    "save_model(vgg19, 'vgg19_hg14.pth')\n",
    "\n",
    "print(\"Training MobileNetV2\")\n",
    "mobilenet_v2 = train_model(mobilenet_v2, train_loader, val_loader, epochs)\n",
    "save_model(mobilenet_v2, 'mobilenet_v2_hg14.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a227cf",
   "metadata": {},
   "source": [
    "# Dirichlet Ensemble Learning\n",
    "\n",
    "When we have multiple models trained on the same task, instead of just picking one or averaging their outputs evenly, we can combine their predictions using weights that come from a Dirichlet distribution.\n",
    "\n",
    "So now, time for testing and combining the three models ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29450ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22864474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "vgg16 = load_model(vgg16, 'vgg16_hg14.pth')\n",
    "vgg19 = load_model(vgg19, 'vgg19_hg14.pth')\n",
    "mobilenet_v2 = load_model(mobilenet_v2, 'mobilenet_v2_hg14.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c943e",
   "metadata": {},
   "source": [
    "Testing models individually is used to compute performances of the different models (we won't do it because we already know how the models perfom in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test each model\n",
    "# acc_vgg16, preds_vgg16, labels = test_model(vgg16, test_loader)\n",
    "# acc_vgg19, preds_vgg19, _ = test_model(vgg19, test_loader)\n",
    "# acc_mobilenet, preds_mobilenet, _ = test_model(mobilenet_v2, test_loader)\n",
    "\n",
    "# print(f\"Test Accuracy - VGG16: {acc_vgg16:.2f}%\")\n",
    "# print(f\"Test Accuracy - VGG19: {acc_vgg19:.2f}%\")\n",
    "# print(f\"Test Accuracy - MobileNetV2: {acc_mobilenet:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21d52c",
   "metadata": {},
   "source": [
    "We can  visualise the confusion matrix, A table used to evaluate the performance of a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf54fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(labels, preds_vgg16)  \n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "# disp.plot()\n",
    "# plt.title(\"Confusion Matrix - VGG16\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_ensemble(models, test_loader, num_runs=10):\n",
    "    all_accuracies = []\n",
    "    final_predictions = None\n",
    "    final_labels = None\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        all_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            probs = []\n",
    "            with torch.no_grad():\n",
    "                for images, _ in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    probs.append(outputs.cpu().numpy())\n",
    "            all_probs.append(np.concatenate(probs, axis=0)) \n",
    "\n",
    "        weights = np.random.dirichlet(np.ones(len(models)))\n",
    "        print(f\"Run {run+1} Dirichlet weights: {weights}\")\n",
    "\n",
    "        weighted_probs = sum(w * p for w, p in zip(weights, all_probs))\n",
    "        predictions = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "        all_labels = []\n",
    "        for _, labels in test_loader:\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, predictions)\n",
    "        print(f\"Run {run+1} Accuracy: {accuracy * 100:.2f}%\")\n",
    "        all_accuracies.append(accuracy * 100)\n",
    "\n",
    "        if run == num_runs - 1:\n",
    "            final_predictions = predictions\n",
    "            final_labels = all_labels\n",
    "  \n",
    "    avg_acc = np.mean(all_accuracies)\n",
    "    std_acc = np.std(all_accuracies)\n",
    "    print(f\"\\nAverage Accuracy over {num_runs} runs: {avg_acc:.2f}% ± {std_acc:.2f}%\")\n",
    "\n",
    "    return all_accuracies, avg_acc, std_acc, final_predictions, final_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1115ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies, avg_acc, std_acc, final_preds, final_labels = dirichlet_ensemble(models, test_loader, num_runs=10)\n",
    "\n",
    "print(f\"Final Ensemble Accuracy: {avg_acc:.2f}% ± {std_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(final_labels, final_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix for Dirichlet Ensemble\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
